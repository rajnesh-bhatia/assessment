{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import col, when, year, current_date, datediff, to_date, year, current_date\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    appName(\"pyspark-1\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Job ID: integer (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Posting Type: string (nullable = true)\n",
      " |-- # Of Positions: integer (nullable = true)\n",
      " |-- Business Title: string (nullable = true)\n",
      " |-- Civil Service Title: string (nullable = true)\n",
      " |-- Title Code No: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Job Category: string (nullable = true)\n",
      " |-- Full-Time/Part-Time indicator: string (nullable = true)\n",
      " |-- Salary Range From: double (nullable = true)\n",
      " |-- Salary Range To: double (nullable = true)\n",
      " |-- Salary Frequency: string (nullable = true)\n",
      " |-- Work Location: string (nullable = true)\n",
      " |-- Division/Work Unit: string (nullable = true)\n",
      " |-- Job Description: string (nullable = true)\n",
      " |-- Minimum Qual Requirements: string (nullable = true)\n",
      " |-- Preferred Skills: string (nullable = true)\n",
      " |-- Additional Information: string (nullable = true)\n",
      " |-- To Apply: string (nullable = true)\n",
      " |-- Hours/Shift: string (nullable = true)\n",
      " |-- Work Location 1: string (nullable = true)\n",
      " |-- Recruitment Contact: string (nullable = true)\n",
      " |-- Residency Requirement: string (nullable = true)\n",
      " |-- Posting Date: string (nullable = true)\n",
      " |-- Post Until: string (nullable = true)\n",
      " |-- Posting Updated: string (nullable = true)\n",
      " |-- Process Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "all_columns = df.columns\n",
    "\n",
    "# Count the occurrences of each column name\n",
    "column_counts = Counter(all_columns)\n",
    "\n",
    "# Filter for names that appear more than once\n",
    "duplicate_columns = [col_name for col_name, count in column_counts.items() if count > 1]\n",
    "\n",
    "print(duplicate_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Utility & Normalizing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, year, current_date, datediff, count, when\n",
    "\n",
    "# 1. Clean salary columns\n",
    "def normalize_salary(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize salary to annual salary.\n",
    "    Assumes 2080 working hours/year for hourly postings.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        \"annual_salary\",\n",
    "        when(col(\"Salary Frequency\")==\"Hourly\", col(\"Salary Range To\")*2080)\n",
    "        .otherwise(col(\"Salary Range To\"))\n",
    "    )\n",
    "\n",
    "# 2. Create average salary column\n",
    "def add_avg_salary(df: DataFrame) -> DataFrame:\n",
    "    return df.withColumn(\"avg_salary\", (col(\"Salary Range From\")+col(\"Salary Range To\"))/2)\n",
    "\n",
    "# 3. Flag degree requirement\n",
    "def add_degree_flag(df: DataFrame) -> DataFrame:\n",
    "    return df.withColumn(\n",
    "        \"requires_degree\",\n",
    "        when(col(\"Minimum Qual Requirements\").rlike(\"Master|PhD\"), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# 4. Add posting recency\n",
    "def add_posting_recency(df: DataFrame) -> DataFrame:\n",
    "    return df.withColumn(\"days_since_posting\", datediff(current_date(), col(\"Posting Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: KPI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI 1: Top 10 job categories\n",
    "def top_job_categories(df: DataFrame) -> DataFrame:\n",
    "    return df.groupBy(\"Job Category\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "\n",
    "# KPI 2: Salary distribution per category\n",
    "def salary_distribution(df: DataFrame) -> DataFrame:\n",
    "    return df.groupBy(\"Job Category\").agg({\"avg_salary\":\"avg\"})\n",
    "\n",
    "# KPI 3: Correlation between degree and salary\n",
    "def degree_salary_correlation(df: DataFrame) -> float:\n",
    "    return df.stat.corr(\"requires_degree\", \"annual_salary\")\n",
    "\n",
    "# KPI 4: Highest salary per agency\n",
    "def highest_salary_per_agency(df: DataFrame) -> DataFrame:\n",
    "    return df.groupBy(\"Agency\").agg({\"annual_salary\":\"max\"})\n",
    "\n",
    "# KPI 5: Average salary per agency (last 2 years)\n",
    "def avg_salary_last2yrs(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    ## converting posting date column to date type first\n",
    "    df = df.withColumn(\"Posting Date\", to_date(col(\"Posting Date\"), \"MM/dd/yyyy\"))\n",
    "    \n",
    "    ## applying logic to get average salary per agency for last 2 years\n",
    "    df_recent = df.withColumn(\"year\", year(col(\"Posting Date\"))).filter(col(\"year\") >= datetime.now().year - 1)\n",
    "    return df_recent.groupBy(\"Agency\").agg({\"avg_salary\":\"avg\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_categories(df: DataFrame):\n",
    "    top10 = df.groupBy(\"Job Category\").count().orderBy(\"count\", ascending=False).limit(10)\n",
    "    pdf = top10.toPandas()\n",
    "    sns.barplot(x=\"Job Category\", y=\"count\", data=pdf)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "def plot_salary_distribution(df: DataFrame):\n",
    "    pdf = df.toPandas()\n",
    "    sns.boxplot(x=\"Job Category\", y=\"avg_salary\", data=pdf)\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have added average salary and annual salary in the dataframe, we can now remove Salary From, Salary To and Salary Frequency\n",
    "def drop_salary_features(df: DataFrame) -> DataFrame:\n",
    "    cols_to_drop = [\"Salary From\", \"Salary To\", \"Salary Frequency\"]\n",
    "    df = df.drop(*cols_to_drop)\n",
    "    return df\n",
    "\n",
    "# Drop sparse columns like those columns with more than 70 or 80 percent null values\n",
    "def drop_sparse_columns(df: DataFrame) -> DataFrame:\n",
    "    threshold = 0.7\n",
    "    total_rows = df.count()\n",
    "    null_counts = df.select([\n",
    "        (count(when(col(c).isNull(), c)) / total_rows).alias(c)\n",
    "        for c in df.columns\n",
    "    ]).collect()[0].asDict()\n",
    "\n",
    "    sparse_columns = [\n",
    "        col_name for col_name, null_ratio in null_counts.items()\n",
    "        if null_ratio > threshold\n",
    "    ]\n",
    "\n",
    "#     df = df.drop(*sparse_columns)\n",
    "    print(sparse_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_salary_normalization(df: DataFrame):\n",
    "    assert df.filter(df[\"Salary Frequency\"]==\"Hourly\").select(\"annual_salary\").first()[0] > 0\n",
    "\n",
    "def test_top_categories(df: DataFrame):\n",
    "    top10 = top_job_categories(df).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df: DataFrame) -> DataFrame:\n",
    "    df = normalize_salary(df)\n",
    "    df = add_avg_salary(df)\n",
    "    df = add_degree_flag(df)\n",
    "    df = add_posting_recency(df)\n",
    "    df = drop_salary_features(df)\n",
    "    df = drop_sparse_columns(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### executing the pipeline using above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-6c7537fd3e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtop_job_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msalary_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correlation:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree_salary_correlation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-972e238d7f2f>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_posting_recency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_salary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_sparse_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-5862df78764f>\u001b[0m in \u001b[0;36mdrop_sparse_columns\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     10\u001b[0m     null_counts = df.select([\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     ]).collect()[0].asDict()\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-5862df78764f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     null_counts = df.select([\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     ]).collect()[0].asDict()\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_rows' is not defined"
     ]
    }
   ],
   "source": [
    "df = process_data(df)\n",
    "\n",
    "top_job_categories(df).show()\n",
    "salary_distribution(df).show()\n",
    "print(\"Correlation:\", degree_salary_correlation(df))\n",
    "highest_salary_per_agency(df).show()\n",
    "avg_salary_last2yrs(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_categories(df)\n",
    "plot_salary_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_frequency(df) -> list:\n",
    "    row_list = df.select('Salary Frequency').distinct().collect()\n",
    "    return [row['Salary Frequency'] for row in row_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = [('A', 'Annual'), ('B', 'Daily')]\n",
    "expected_result = ['Annual', 'Daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_salary_frequency(mock_data: list, \n",
    "                              expected_result: list,\n",
    "                              schema: list = ['id', 'Salary Frequency']):  \n",
    "    mock_df = spark.createDataFrame(data = mock_data, schema = schema)\n",
    "    assert get_salary_frequency(mock_df) == expected_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_salary_frequency(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
